{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsii4VOlan4g"
      },
      "source": [
        "# V-JEPA 2 Demo Notebook\n",
        "\n",
        "This tutorial provides an example of how to load the V-JEPA 2 model in vanilla PyTorch and HuggingFace, extract a video embedding, and then predict an action class. For more details about the paper and model weights, please see https://github.com/facebookresearch/vjepa2."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDuyecM8apc3",
        "outputId": "be702939-a215-49b1-ac3a-867505c0fb65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceq0YPTGan4h"
      },
      "source": [
        "First, let's import the necessary libraries and load the necessary functions for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U decord"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh-yrjyXaxAu",
        "outputId": "64fe5f07-45f8-49e8-9367-dab24bbcc696"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from decord) (2.0.2)\n",
            "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: decord\n",
            "Successfully installed decord-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CV0d-VQKan4h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Add the vjepa2-main directory to the Python path\n",
        "sys.path.append('/content/drive/MyDrive/vjepa2-main')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from decord import VideoReader\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "\n",
        "import src.datasets.utils.video.transforms as video_transforms\n",
        "import src.datasets.utils.video.volume_transforms as volume_transforms\n",
        "from src.models.attentive_pooler import AttentiveClassifier\n",
        "from src.models.vision_transformer import vit_large_rope\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "def load_pretrained_vjepa_pt_weights(model, pretrained_weights):\n",
        "    # Load weights of the VJEPA2 encoder\n",
        "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
        "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"encoder\"]\n",
        "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    pretrained_dict = {k.replace(\"backbone.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
        "\n",
        "\n",
        "def load_pretrained_vjepa_classifier_weights(model, pretrained_weights):\n",
        "    # Load weights of the VJEPA2 classifier\n",
        "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
        "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
        "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
        "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
        "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
        "\n",
        "\n",
        "def build_pt_video_transform(img_size):\n",
        "    short_side_size = int(256.0 / 224 * img_size)\n",
        "    # Eval transform has no random cropping nor flip\n",
        "    eval_transform = video_transforms.Compose(\n",
        "        [\n",
        "            video_transforms.Resize(short_side_size, interpolation=\"bilinear\"),\n",
        "            video_transforms.CenterCrop(size=(img_size, img_size)),\n",
        "            volume_transforms.ClipToTensor(),\n",
        "            video_transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
        "        ]\n",
        "    )\n",
        "    return eval_transform\n",
        "\n",
        "\n",
        "def get_video():\n",
        "    vr = VideoReader(\"center_libx264_yuv420.mp4\")\n",
        "    # choosing some frames here, you can define more complex sampling strategy\n",
        "    frame_idx = np.arange(0, 128, 2)\n",
        "    video = vr.get_batch(frame_idx).asnumpy()\n",
        "    return video\n",
        "\n",
        "\n",
        "def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform):\n",
        "    # Run a sample inference with VJEPA\n",
        "    with torch.inference_mode():\n",
        "        # Read and pre-process the image\n",
        "        video = get_video()  # T x H x W x C\n",
        "        video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
        "        x_pt = pt_transform(video).cuda().unsqueeze(0)\n",
        "        x_hf = hf_transform(video, return_tensors=\"pt\")[\"pixel_values_videos\"].to(\"cuda\")\n",
        "        # Extract the patch-wise features from the last layer\n",
        "        out_patch_features_pt = model_pt(x_pt)\n",
        "        out_patch_features_hf = model_hf.get_vision_features(x_hf)\n",
        "\n",
        "    return out_patch_features_hf, out_patch_features_pt\n",
        "\n",
        "\n",
        "def get_vjepa_video_classification_results(classifier, out_patch_features_pt):\n",
        "    SOMETHING_SOMETHING_V2_CLASSES = json.load(open(\"ssv2_classes.json\", \"r\"))\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out_classifier = classifier(out_patch_features_pt)\n",
        "\n",
        "    print(f\"Classifier output shape: {out_classifier.shape}\")\n",
        "\n",
        "    print(\"Top 5 predicted class names:\")\n",
        "    top5_indices = out_classifier.topk(5).indices[0]\n",
        "    top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n",
        "    for idx, prob in zip(top5_indices, top5_probs):\n",
        "        str_idx = str(idx.item())\n",
        "        print(f\"{SOMETHING_SOMETHING_V2_CLASSES[str_idx]} ({prob}%)\")\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ieKhsMlan4h"
      },
      "source": [
        "Next, let's download a sample video to the local repository. If the video is already downloaded, the code will skip this step. Likewise, let's download a mapping for the action recognition classes used in Something-Something V2, so we can interpret the predicted action class from our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SV9I_nxan4i",
        "outputId": "54918a16-1e0d-4178-fa1f-95050076635f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SSV2 classes\n"
          ]
        }
      ],
      "source": [
        "# sample_video_path = \"sample_video.mp4\"\n",
        "# # Download the video if not yet downloaded to local path\n",
        "# if not os.path.exists(sample_video_path):\n",
        "#     video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4\"\n",
        "#     command = [\"wget\", video_url, \"-O\", sample_video_path]\n",
        "#     subprocess.run(command)\n",
        "#     print(\"Downloading video\")\n",
        "\n",
        "# Download SSV2 classes if not already present\n",
        "ssv2_classes_path = \"ssv2_classes.json\"\n",
        "if not os.path.exists(ssv2_classes_path):\n",
        "    command = [\n",
        "        \"wget\",\n",
        "        \"https://huggingface.co/datasets/huggingface/label-files/resolve/d79675f2d50a7b1ecf98923d42c30526a51818e2/\"\n",
        "        \"something-something-v2-id2label.json\",\n",
        "        \"-O\",\n",
        "        \"ssv2_classes.json\",\n",
        "    ]\n",
        "    subprocess.run(command)\n",
        "    print(\"Downloading SSV2 classes\")\n",
        "sample_video_path = \"/content/center_libx264_yuv420.mp4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RFRhFUYan4i"
      },
      "source": [
        "Now, let's load the models in both vanilla Pytorch as well as through the HuggingFace API. Note that HuggingFace API will automatically load the weights through `from_pretrained()`, so there is no additional download required for HuggingFace.\n",
        "\n",
        "To download the PyTorch model weights, use wget and specify your preferred target path. See the README for the model weight URLs.\n",
        "E.g.\n",
        "```\n",
        "wget https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt -P YOUR_DIR\n",
        "```\n",
        "Then update `pt_model_path` with `YOUR_DIR/vitg-384.pt`. Also note that you have the option to use `torch.hub.load`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !wget https://dl.fbaipublicfiles.com/vjepa2/vitl.pt -P weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpYhmWmby54",
        "outputId": "302bfe04-61ec-49ba-8705-5abbbb5a2d09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-31 18:54:26--  https://dl.fbaipublicfiles.com/vjepa2/vitl.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.14, 3.163.189.51, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5127726842 (4.8G) [application/xml]\n",
            "Saving to: ‘weights/vitl.pt’\n",
            "\n",
            "vitl.pt             100%[===================>]   4.78G   101MB/s    in 67s     \n",
            "\n",
            "2025-10-31 18:55:34 (72.8 MB/s) - ‘weights/vitl.pt’ saved [5127726842/5127726842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exTivefqan4i",
        "outputId": "93486fc4-3a70-4729-9818-ed634a9ec172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights found at weights/vitl.pt and loaded with msg: <All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "# HuggingFace model repo name\n",
        "hf_model_name = (\n",
        "    \"facebook/vjepa2-vitl-fpc64-256\"  # Replace with your favored model, e.g. facebook/vjepa2-vitg-fpc64-384\n",
        ")\n",
        "# Path to local PyTorch weights\n",
        "pt_model_path = \"weights/vitl.pt\"\n",
        "img_size = 256\n",
        "# # Initialize the HuggingFace model, load pretrained weights\n",
        "model_hf = AutoModel.from_pretrained(hf_model_name)\n",
        "model_hf.cuda().eval()\n",
        "\n",
        "# # Build HuggingFace preprocessing transform\n",
        "hf_transform = AutoVideoProcessor.from_pretrained(hf_model_name)\n",
        "img_size = hf_transform.crop_size[\"height\"]  # E.g. 384, 256, etc.\n",
        "\n",
        "# Initialize the PyTorch model, load pretrained weights\n",
        "model_pt = vit_large_rope(img_size=(img_size, img_size), num_frames=64)\n",
        "model_pt.cuda().eval()\n",
        "load_pretrained_vjepa_pt_weights(model_pt, pt_model_path)\n",
        "\n",
        "### Can also use torch.hub to load the model\n",
        "# model_pt, _ = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_giant_384')\n",
        "# model_pt.cuda().eval()\n",
        "\n",
        "# Build PyTorch preprocessing transform\n",
        "pt_video_transform = build_pt_video_transform(img_size=img_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEbEYBFzan4i"
      },
      "source": [
        "Now we can run the encoder on the video to get the patch-wise features from the last layer of the encoder. To verify that the HuggingFace and PyTorch models are equivalent, we will compare the values of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIxEqfQ4an4i",
        "outputId": "922e6620-491c-4331-90d5-e1af534f48fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Inference results on video:\n",
            "    HuggingFace output shape: torch.Size([1, 8192, 1024])\n",
            "    PyTorch output shape:     torch.Size([1, 8192, 1024])\n",
            "    Absolute difference sum:  1993360.000000\n",
            "    Close: False\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Inference on video to get the patch-wise features\n",
        "out_patch_features_hf, out_patch_features_pt = forward_vjepa_video(\n",
        "    model_hf, model_pt, hf_transform, pt_video_transform\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"\"\"\n",
        "    Inference results on video:\n",
        "    HuggingFace output shape: {out_patch_features_hf.shape}\n",
        "    PyTorch output shape:     {out_patch_features_pt.shape}\n",
        "    Absolute difference sum:  {torch.abs(out_patch_features_pt - out_patch_features_hf).sum():.6f}\n",
        "    Close: {torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3)}\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW6T8qOLan4i"
      },
      "source": [
        "Great! Now we know that the features from both models are equivalent. Now let's run a pretrained attentive probe classifier on top of the extracted features, to predict an action class for the video. Let's use the Something-Something V2 probe. Note that the repository also includes attentive probe weights for other evaluations such as EPIC-KITCHENS-100 and Diving48.\n",
        "\n",
        "To download the attentive probe weights, use wget and specify your preferred target path. E.g. `wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P YOUR_DIR`\n",
        "\n",
        "Then update `classifier_model_path` with `YOUR_DIR/ssv2-vitg-384-64x2x3.pt`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitl-16x2x3.pt -P probe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNygryF4jBk2",
        "outputId": "e0452810-ded4-4bfc-fd59-54d4cf247571"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-31 18:58:35--  https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitl-16x2x3.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.14, 3.163.189.108, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 198078572 (189M) [application/vnd.snesdev-page-table]\n",
            "Saving to: ‘probe/ssv2-vitl-16x2x3.pt’\n",
            "\n",
            "ssv2-vitl-16x2x3.pt 100%[===================>] 188.90M   326MB/s    in 0.6s    \n",
            "\n",
            "2025-10-31 18:58:36 (326 MB/s) - ‘probe/ssv2-vitl-16x2x3.pt’ saved [198078572/198078572]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruYR63Guan4i",
        "outputId": "44d16892-4ee1-475c-83ac-cabd1021d014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained weights found at probe/ssv2-vitl-16x2x3.pt and loaded with msg: <All keys matched successfully>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier output shape: torch.Size([1, 174])\n",
            "Top 5 predicted class names:\n",
            "Showing that [something] is empty (31.374454498291016%)\n",
            "Moving [part] of [something] (23.622928619384766%)\n",
            "Turning the camera left while filming [something] (19.779525756835938%)\n",
            "Showing [something] behind [something] (13.440799713134766%)\n",
            "Showing [something] to the camera (11.782286643981934%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-961611305.py:89: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n"
          ]
        }
      ],
      "source": [
        "# Initialize the classifier\n",
        "classifier_model_path = \"probe/ssv2-vitl-16x2x3.pt\"\n",
        "classifier = (\n",
        "    AttentiveClassifier(embed_dim=model_pt.embed_dim, num_heads=16, depth=4, num_classes=174).cuda().eval()\n",
        ")\n",
        "load_pretrained_vjepa_classifier_weights(classifier, classifier_model_path)\n",
        "\n",
        "# Get classification results\n",
        "get_vjepa_video_classification_results(classifier, out_patch_features_pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqXT2E2Lan4i"
      },
      "source": [
        "The video features a man putting a bowling ball into a tube, so the predicted action of \"Putting [something] into [something]\" makes sense!\n",
        "\n",
        "This concludes the tutorial. Please see the README and paper for full details on the capabilities of V-JEPA 2 :)"
      ]
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "f0b70ba6-1c84-47e1-81bd-b7642f9acf50",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}